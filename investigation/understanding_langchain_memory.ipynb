{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import feedparser\n",
    "\n",
    "KEYWORD = \"RAG\"\n",
    "# ArXiv API URL\n",
    "url = f'http://export.arxiv.org/api/query?search_query=abs:{KEYWORD}&start=0&max_results=10&sortBy=lastUpdatedDate&sortOrder=descending'\n",
    "# Parse the API response\n",
    "feed = feedparser.parse(url)\n",
    "data = feed.entries\n",
    "\n",
    "docs = []\n",
    "for paper in data:\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"summary\"]\n",
    "    link = paper[\"link\"]\n",
    "    paper_content = f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    paper_content = paper_content.lower()\n",
    "\n",
    "    docs.append(Document(page_content=paper_content,\n",
    "                            metadata={\"link\": link}))\n",
    "    \n",
    "embedding_model = CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=docs, embedding=embedding_model)\n",
    "\n",
    "vector_db.add_documents(docs)\n",
    "\n",
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're an assistant who's good at {context}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You're an assistant who's good at {context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "llm = ChatCohere(model=\"command-r-plus-08-2024\",\n",
    "           max_tokens=256,\n",
    "           temperature=0.5\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# NOTE: default input vars must be 'context' and 'input' for the prompt\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a concept in computer science and artificial intelligence, particularly relevant to the field of retrieval-augmented generation (RAG) and large language models (LLMs). It is a technique used to break down complex tasks into smaller, more manageable sub-tasks or steps.\\n\\nIn the context of the research paper you provided, titled \"pike-rag: specialized knowledge and rationale augmented generation,\" task decomposition is specifically mentioned as \"knowledge-aware task decomposition.\" This technique is proposed to address the challenges of extracting and applying specialized knowledge from diverse industrial applications.\\n\\nKnowledge-aware task decomposition involves the following steps:\\n1. Identifying the main task or problem that needs to be solved.\\n2. Analyzing the task to understand its complexity and the knowledge requirements.\\n3. Decomposing the main task into a set of smaller, more focused sub-tasks or steps. Each sub-task should be associated with specific knowledge or information needs.\\n4. Extracting relevant knowledge or information from available data sources or corpora for each sub-task. This process is referred to as \"knowledge atomizing,\" where multifaceted knowledge is extracted from data chunks.\\n5. Iteratively constructing a rationale or reasoning process by combining the extracted knowledge with the original query or problem statement. This rationale helps guide the LLM towards generating accurate responses.\\n\\nBy breaking down complex tasks into smaller, knowledge-specific sub-tasks, the system can more effectively retrieve and apply the required information. This approach enhances the performance of RAG systems in handling intricate industrial tasks that demand deep domain-specific knowledge and logical reasoning.\\n\\nTask decomposition is a powerful strategy to improve the problem-solving capabilities of LLMs and RAG systems, making them more adaptable to the complex and diverse needs of real-world applications.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "system_prompt_to_reformulate_input = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference the context in the chat history \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it is needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "prompt_to_reformulate_input = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_to_reformulate_input),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "history_aware_retriever_chain = create_history_aware_retriever(\n",
    "    llm, retriever, prompt_to_reformulate_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\"You are an AI assistant called 'ArXiv Assist' that has a conversation with the user and answers to questions based on the provided research papers. \"\n",
    "                 \"There could be cases when user does not ask a question, but it is just a statement. In such cases, do not use knowledge from the papers. Just reply back normally and accordingly to have a good conversation (e.g. 'You're welcome' if the input is 'Thanks'). \"\n",
    "                 \"If no relevant information is found in the papers, respond with: 'Sorry, I do not have much information related to this. \"\n",
    "                 \"If you reference a paper, provide a hyperlink to it. \"\n",
    "                 \"Be polite, friendly, and format your response well (e.g., use bullet points, bold text, etc.). \"\n",
    "                 \"Below are relevant excerpts from the research papers:\\n{context}\\n\\n\"\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
